---
title: "りんなAIモデル一覧"
emoji: "🎉"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: []
published: false
---

## gpt-japanese
## 1b
1.3B パラメータの日本語 GPT モデル
#### モデルアーキテクチャ
24 層、2048 隠れサイズのトランスフォーマーベースの言語モデル。

#### トレーニング
モデルは、従来の言語モデリング目標を最適化するために、日本語 C4、日本語 CC-100、および日本語 Wikipediaでトレーニングされました。同じデータから選択された検証セットでは約 14 パープレキシティに達します。
## 2b
### xsmall
超小型の日本語 GPT-2 モデル

#### モデルアーキテクチャ
6 層、512 隠れサイズのトランスフォーマーベースの言語モデル。

#### トレーニング
このモデルは、8\*V100 GPU で従来の言語モデリング目標を最適化するために、日本語 CC-100と日本語 Wikipediaで約 4 日間トレーニングされました。CC-100 から選択された検証セットでは約 28 の複雑度に達します。

### small
小型の日本語 GPT-2 モデル

#### モデルアーキテクチャ
12 層、768 隠れサイズのトランスフォーマーベースの言語モデル。

#### トレーニング
このモデルは、8\*V100 GPU で従来の言語モデリング目標を最適化するために、日本語 CC-100と日本語 Wikipediaで約 15 日間トレーニングされました。CC-100 から選択された検証セットでは約 21 の複雑度に達します。

### medium
中型の日本語 GPT-2 モデル

#### モデルアーキテクチャ
24 層、1024 隠れサイズのトランスフォーマーベースの言語モデル。

#### トレーニング
このモデルは、8\*V100 GPU で従来の言語モデリング目標を最適化するために、日本語 CC-100と日本語 Wikipediaで約 30 日間トレーニングされました。同じデータから選択された検証セットでは約 18 パープレキシティに達します。

## neox
### small
小型の日本語 GPT-NeoX モデルを提供します。

#### モデルアーキテクチャ
12 層、768 隠れサイズのトランスフォーマーベースの言語モデル。

#### トレーニング
モデルは、従来の言語モデリング目標を最適化するために、日本語 CC-100、日本語 C4、および日本語 Wikipediaでトレーニングされました。

### 3.6b
36 億パラメータの日本語 GPT-NeoX モデル

#### モデルアーキテクチャモデルアーキテクチャ

36 層、2816 隠れサイズのトランスフォーマーベースの言語モデル。

#### 事前トレーニング

このモデルは、従来の言語モデリング目標を最適化するために、日本語 CC-100、日本語 C4、および日本語 Wikipediaからの約3 億 1,250 億のトークンでトレーニングされました。

最終的な検証の複雑さは8.68に達しました。

## gpt-bilingual
### 4b
38 億パラメータの英語と日本語のバイリンガル GPT-NeoX モデル

#### モデルアーキテクチャ

36 層、2816 隠れサイズのトランスフォーマーベースの言語モデル。

#### 事前トレーニング

モデルは、次のコーパスの混合物からの約524Bトークンでトレーニングされました。

日本のCC-100、日本語 C4、ザ・パイル、レッドパジャマ、ウィキペディア

### 4b-8k
注意: このモデルはtransformers>=4.31.0正しく動作する必要があります。

38 億パラメータの英語と日本語のバイリンガル GPT-NeoX モデル

#### モデルアーキテクチャ

36 層、2816 隠れサイズのトランスフォーマーベースの言語モデル。

#### Fine-tuning

モデルは、次のように、トレーニング前のコーパスからサンプリングされた長いシーケンス (4000 トークンを超える) でトレーニングされました。微調整データには合計15 億のトークンが含まれています。

日本のCC-100、日本語 C4、ザ・パイル、レッドパジャマ、ウィキペディア