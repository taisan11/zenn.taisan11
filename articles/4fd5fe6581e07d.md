---
title: "りんなAIモデル一覧"
emoji: "🎉"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: []
published: false
---

## gpt
## 1b
1.3B パラメータの日本語 GPT モデル
#### モデルアーキテクチャ
24 層、2048 隠れサイズのトランスフォーマーベースの言語モデル。

#### トレーニング
モデルは、従来の言語モデリング目標を最適化するために、日本語 C4、日本語 CC-100、および日本語 Wikipediaでトレーニングされました。同じデータから選択された検証セットでは約 14 パープレキシティに達します。
